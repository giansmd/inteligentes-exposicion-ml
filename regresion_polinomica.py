"""
Regresi√≥n Polin√≥mica - California Housing
Aplicaci√≥n Streamlit para predicci√≥n de precios de viviendas
usando regresi√≥n polin√≥mica.

Autor: Persona 3 - Equipo de Algoritmos de Regresi√≥n
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import streamlit as st
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# ============================================
# CONFIGURACI√ìN DE LA P√ÅGINA
# ============================================
st.set_page_config(
    page_title="Regresi√≥n Polin√≥mica - California Housing",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.title("üè† Regresi√≥n Polin√≥mica - California Housing")
st.markdown("Predicci√≥n del precio medio de viviendas usando caracter√≠sticas polin√≥micas")
st.markdown("---")

# ============================================
# FUNCIONES CON CACH√â
# ============================================
@st.cache_data
def load_data():
    """Carga el dataset California Housing."""
    data = fetch_california_housing(as_frame=True)
    return data.data, data.target, data.feature_names, data.target_names[0]

@st.cache_resource
def train_polynomial_model(_X_train, _y_train, degree):
    """Entrena el modelo de regresi√≥n polin√≥mica."""
    poly = PolynomialFeatures(degree=degree, include_bias=False)
    X_train_poly = poly.fit_transform(_X_train)
    model = LinearRegression()
    model.fit(X_train_poly, _y_train)
    return model, poly

# ============================================
# CARGA DE DATOS
# ============================================
X, y, feature_names, target_name = load_data()

# ============================================
# BARRA LATERAL - CONFIGURACI√ìN
# ============================================
st.sidebar.header("‚öôÔ∏è Configuraci√≥n del Modelo")
st.sidebar.markdown("---")

test_size = st.sidebar.slider(
    "Porcentaje de datos para test (%)", 
    min_value=10, 
    max_value=50, 
    value=20, 
    step=5
) / 100

random_state = st.sidebar.number_input(
    "Semilla aleatoria", 
    min_value=0, 
    max_value=1000, 
    value=42, 
    step=1
)

degree = st.sidebar.slider(
    "Grado del polinomio", 
    min_value=1, 
    max_value=4, 
    value=2, 
    step=1,
    help="Grados m√°s altos capturan relaciones m√°s complejas pero pueden causar sobreajuste"
)

st.sidebar.markdown("---")
st.sidebar.info("""
**Nota sobre el grado:**
- Grado 1 = Regresi√≥n Lineal
- Grado 2 = T√©rminos cuadr√°ticos
- Grado 3+ = Mayor complejidad
""")

# ============================================
# VISTA PREVIA DE LOS DATOS
# ============================================
st.subheader("üìä Vista Previa de los Datos")

col1, col2 = st.columns(2)

with col1:
    st.write("**Variables de entrada (X)**")
    st.dataframe(X.head(20), use_container_width=True)
    st.write(f"üìê Dimensiones: `{X.shape[0]}` filas √ó `{X.shape[1]}` columnas")

with col2:
    st.write(f"**Variable objetivo ({target_name})**")
    st.dataframe(pd.DataFrame({target_name: y}).head(20), use_container_width=True)
    st.write(f"üìê Dimensiones: `{y.shape[0]}` valores")

st.markdown("---")

# ============================================
# DIVISI√ìN TRAIN / TEST
# ============================================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=test_size, random_state=random_state
)

st.subheader("üìÇ Divisi√≥n de Datos")
col1, col2, col3 = st.columns(3)
col1.metric("Total de muestras", f"{len(X):,}")
col2.metric("Muestras de entrenamiento", f"{len(X_train):,}")
col3.metric("Muestras de prueba", f"{len(X_test):,}")

st.markdown("---")

# ============================================
# TRANSFORMACI√ìN POLIN√ìMICA
# ============================================
st.subheader("üîÑ Transformaci√≥n Polin√≥mica")

# Crear transformador y aplicar
poly = PolynomialFeatures(degree=degree, include_bias=False)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

col1, col2, col3 = st.columns(3)
col1.metric("Grado seleccionado", degree)
col2.metric("Caracter√≠sticas originales", X_train.shape[1])
col3.metric("Caracter√≠sticas transformadas", X_train_poly.shape[1])

# Mostrar ejemplo de nombres de caracter√≠sticas generadas
with st.expander("üîç Ver nombres de caracter√≠sticas generadas"):
    feature_names_poly = poly.get_feature_names_out(feature_names)
    st.write(f"Se generaron **{len(feature_names_poly)}** caracter√≠sticas:")
    # Mostrar solo las primeras 20 para no saturar
    st.write(list(feature_names_poly[:20]))
    if len(feature_names_poly) > 20:
        st.write(f"... y {len(feature_names_poly) - 20} m√°s")

st.markdown("---")

# ============================================
# ENTRENAMIENTO DEL MODELO
# ============================================
st.subheader("üéØ Entrenamiento del Modelo")

with st.spinner("Entrenando modelo de regresi√≥n polin√≥mica..."):
    model = LinearRegression()
    model.fit(X_train_poly, y_train)

st.success("‚úÖ Modelo entrenado exitosamente")

# ============================================
# PREDICCIONES
# ============================================
y_pred_train = model.predict(X_train_poly)
y_pred_test = model.predict(X_test_poly)

# ============================================
# M√âTRICAS DE EVALUACI√ìN
# ============================================
st.subheader("üìà M√©tricas de Evaluaci√≥n")

r2_train = r2_score(y_train, y_pred_train)
r2_test = r2_score(y_test, y_pred_test)
mse_train = mean_squared_error(y_train, y_pred_train)
mse_test = mean_squared_error(y_test, y_pred_test)
rmse_train = np.sqrt(mse_train)
rmse_test = np.sqrt(mse_test)

col1, col2 = st.columns(2)

with col1:
    st.markdown("**Conjunto de Entrenamiento:**")
    subcol1, subcol2, subcol3 = st.columns(3)
    subcol1.metric("R¬≤", f"{r2_train:.4f}")
    subcol2.metric("MSE", f"{mse_train:.4f}")
    subcol3.metric("RMSE", f"{rmse_train:.4f}")

with col2:
    st.markdown("**Conjunto de Prueba:**")
    subcol1, subcol2, subcol3 = st.columns(3)
    subcol1.metric("R¬≤", f"{r2_test:.4f}")
    subcol2.metric("MSE", f"{mse_test:.4f}")
    subcol3.metric("RMSE", f"{rmse_test:.4f}")

# Detecci√≥n de sobreajuste
st.markdown("---")
diff_r2 = r2_train - r2_test

if diff_r2 > 0.15:
    st.error(f"""
    ‚ö†Ô∏è **Sobreajuste detectado**
    
    La diferencia entre R¬≤ de entrenamiento ({r2_train:.4f}) y R¬≤ de prueba ({r2_test:.4f}) 
    es de {diff_r2:.4f}, lo cual indica que el modelo est√° memorizando los datos de 
    entrenamiento en lugar de generalizar. Considera reducir el grado del polinomio.
    """)
elif diff_r2 > 0.1:
    st.warning(f"""
    ‚ö†Ô∏è **Posible sobreajuste**
    
    La diferencia entre R¬≤ de entrenamiento y prueba es de {diff_r2:.4f}. 
    Monitorea el rendimiento con diferentes grados de polinomio.
    """)
elif r2_test < 0.5:
    st.warning("""
    ‚ö†Ô∏è **Bajo poder predictivo**
    
    El modelo tiene un R¬≤ menor a 0.5 en el conjunto de prueba, 
    lo que indica que explica menos del 50% de la variabilidad de los datos.
    """)
else:
    st.success(f"""
    ‚úÖ **El modelo generaliza correctamente**
    
    R¬≤ en entrenamiento: {r2_train:.4f}
    R¬≤ en prueba: {r2_test:.4f}
    Diferencia: {diff_r2:.4f}
    """)

st.markdown("---")

# ============================================
# TABLA DE PREDICCIONES
# ============================================
st.subheader("üìã Predicciones vs Valores Reales")

df_result = pd.DataFrame({
    "Real": y_test.values,
    "Predicho": y_pred_test,
    "Error": y_test.values - y_pred_test,
    "Error Absoluto": np.abs(y_test.values - y_pred_test),
    "Error Porcentual (%)": np.abs((y_test.values - y_pred_test) / y_test.values) * 100
})

# Estad√≠sticas de error
col1, col2, col3 = st.columns(3)
col1.metric("Error Medio Absoluto", f"{df_result['Error Absoluto'].mean():.4f}")
col2.metric("Error M√°ximo", f"{df_result['Error Absoluto'].max():.4f}")
col3.metric("Error M√≠nimo", f"{df_result['Error Absoluto'].min():.4f}")

st.dataframe(df_result.head(20).style.format({
    "Real": "{:.4f}",
    "Predicho": "{:.4f}",
    "Error": "{:.4f}",
    "Error Absoluto": "{:.4f}",
    "Error Porcentual (%)": "{:.2f}%"
}), use_container_width=True)

st.markdown("---")

# ============================================
# VISUALIZACIONES
# ============================================
st.subheader("üìä Visualizaciones")

tab1, tab2, tab3 = st.tabs(["Real vs Predicho", "Distribuci√≥n de Errores", "Comparaci√≥n de Grados"])

# Tab 1: Gr√°fico Real vs Predicho
with tab1:
    fig1, ax1 = plt.subplots(figsize=(10, 6))
    
    scatter = ax1.scatter(y_test, y_pred_test, alpha=0.5, c=df_result["Error Absoluto"], 
                         cmap='coolwarm', edgecolors='k', linewidth=0.3)
    
    # L√≠nea de predicci√≥n perfecta
    min_val = min(y_test.min(), y_pred_test.min())
    max_val = max(y_test.max(), y_pred_test.max())
    ax1.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, 
             label='Predicci√≥n perfecta (y=x)')
    
    # L√≠nea de tendencia
    z = np.polyfit(y_test, y_pred_test, 1)
    p = np.poly1d(z)
    x_line = np.linspace(y_test.min(), y_test.max(), 100)
    ax1.plot(x_line, p(x_line), 'g-', lw=2, 
             label=f'Tendencia: y = {z[0]:.3f}x + {z[1]:.3f}')
    
    ax1.set_xlabel("Valor Real", fontsize=12)
    ax1.set_ylabel("Valor Predicho", fontsize=12)
    ax1.set_title(f"Regresi√≥n Polin√≥mica (Grado {degree}): Real vs Predicho\nR¬≤ = {r2_test:.4f}", 
                  fontsize=14)
    ax1.legend(loc='upper left')
    ax1.grid(True, alpha=0.3)
    
    # Colorbar
    cbar = plt.colorbar(scatter, ax=ax1)
    cbar.set_label('Error Absoluto')
    
    plt.tight_layout()
    st.pyplot(fig1)

# Tab 2: Distribuci√≥n de Errores
with tab2:
    fig2, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Histograma de errores
    axes[0].hist(df_result["Error"], bins=50, edgecolor='black', alpha=0.7, color='steelblue')
    axes[0].axvline(x=0, color='r', linestyle='--', linewidth=2, label='Error = 0')
    axes[0].axvline(x=df_result["Error"].mean(), color='g', linestyle='-', linewidth=2, 
                    label=f'Media = {df_result["Error"].mean():.4f}')
    axes[0].set_xlabel("Error de Predicci√≥n", fontsize=12)
    axes[0].set_ylabel("Frecuencia", fontsize=12)
    axes[0].set_title("Distribuci√≥n de Errores", fontsize=14)
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # Boxplot de errores
    axes[1].boxplot(df_result["Error"], vert=True)
    axes[1].axhline(y=0, color='r', linestyle='--', linewidth=2)
    axes[1].set_ylabel("Error de Predicci√≥n", fontsize=12)
    axes[1].set_title("Boxplot de Errores", fontsize=14)
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    st.pyplot(fig2)
    
    # Estad√≠sticas adicionales
    st.markdown("**Estad√≠sticas de los Errores:**")
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("Media", f"{df_result['Error'].mean():.4f}")
    col2.metric("Desviaci√≥n Est√°ndar", f"{df_result['Error'].std():.4f}")
    col3.metric("Mediana", f"{df_result['Error'].median():.4f}")
    col4.metric("Rango Intercuart√≠lico", f"{df_result['Error'].quantile(0.75) - df_result['Error'].quantile(0.25):.4f}")

# Tab 3: Comparaci√≥n de Grados
with tab3:
    st.write("Comparaci√≥n del rendimiento con diferentes grados polin√≥micos:")
    
    degrees_to_compare = [1, 2, 3, 4]
    results_comparison = []
    
    progress_bar = st.progress(0)
    
    for i, d in enumerate(degrees_to_compare):
        poly_temp = PolynomialFeatures(degree=d, include_bias=False)
        X_train_temp = poly_temp.fit_transform(X_train)
        X_test_temp = poly_temp.transform(X_test)
        
        model_temp = LinearRegression()
        model_temp.fit(X_train_temp, y_train)
        
        y_pred_train_temp = model_temp.predict(X_train_temp)
        y_pred_test_temp = model_temp.predict(X_test_temp)
        
        r2_train_temp = r2_score(y_train, y_pred_train_temp)
        r2_test_temp = r2_score(y_test, y_pred_test_temp)
        mse_test_temp = mean_squared_error(y_test, y_pred_test_temp)
        
        results_comparison.append({
            "Grado": d,
            "R¬≤ Train": r2_train_temp,
            "R¬≤ Test": r2_test_temp,
            "Diferencia R¬≤": r2_train_temp - r2_test_temp,
            "MSE Test": mse_test_temp,
            "RMSE Test": np.sqrt(mse_test_temp),
            "Caracter√≠sticas": X_train_temp.shape[1]
        })
        
        progress_bar.progress((i + 1) / len(degrees_to_compare))
    
    df_comparison = pd.DataFrame(results_comparison)
    
    st.dataframe(df_comparison.style.format({
        "R¬≤ Train": "{:.4f}",
        "R¬≤ Test": "{:.4f}",
        "Diferencia R¬≤": "{:.4f}",
        "MSE Test": "{:.4f}",
        "RMSE Test": "{:.4f}",
        "Caracter√≠sticas": "{:,}"
    }).highlight_max(subset=["R¬≤ Test"], color='lightgreen')
      .highlight_min(subset=["MSE Test", "Diferencia R¬≤"], color='lightgreen'),
    use_container_width=True)
    
    # Gr√°fico de comparaci√≥n
    fig3, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # R¬≤ por grado
    x_pos = np.arange(len(degrees_to_compare))
    width = 0.35
    
    bars1 = axes[0].bar(x_pos - width/2, df_comparison["R¬≤ Train"], width, label='R¬≤ Train', color='steelblue')
    bars2 = axes[0].bar(x_pos + width/2, df_comparison["R¬≤ Test"], width, label='R¬≤ Test', color='coral')
    
    axes[0].set_xlabel("Grado del Polinomio", fontsize=12)
    axes[0].set_ylabel("R¬≤", fontsize=12)
    axes[0].set_title("Comparaci√≥n de R¬≤ por Grado Polin√≥mico", fontsize=14)
    axes[0].set_xticks(x_pos)
    axes[0].set_xticklabels(degrees_to_compare)
    axes[0].legend()
    axes[0].grid(True, alpha=0.3, axis='y')
    
    # MSE y n√∫mero de caracter√≠sticas
    ax_twin = axes[1].twinx()
    
    line1, = axes[1].plot(degrees_to_compare, df_comparison["MSE Test"], 'b-o', 
                          linewidth=2, markersize=8, label='MSE Test')
    line2, = ax_twin.plot(degrees_to_compare, df_comparison["Caracter√≠sticas"], 'r-s', 
                          linewidth=2, markersize=8, label='Caracter√≠sticas')
    
    axes[1].set_xlabel("Grado del Polinomio", fontsize=12)
    axes[1].set_ylabel("MSE Test", fontsize=12, color='blue')
    ax_twin.set_ylabel("N√∫mero de Caracter√≠sticas", fontsize=12, color='red')
    axes[1].set_title("MSE y Complejidad del Modelo", fontsize=14)
    axes[1].tick_params(axis='y', labelcolor='blue')
    ax_twin.tick_params(axis='y', labelcolor='red')
    
    lines = [line1, line2]
    labels = [l.get_label() for l in lines]
    axes[1].legend(lines, labels, loc='upper left')
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    st.pyplot(fig3)
    
    # Recomendaci√≥n
    best_degree = df_comparison.loc[df_comparison["R¬≤ Test"].idxmax(), "Grado"]
    st.info(f"""
    **Recomendaci√≥n:** Basado en el R¬≤ del conjunto de prueba, el grado **{best_degree}** 
    ofrece el mejor balance entre ajuste y generalizaci√≥n para este dataset.
    """)

st.markdown("---")

# ============================================
# EXPORTAR PREDICCIONES
# ============================================
st.subheader("üíæ Exportar Resultados")

col1, col2 = st.columns(2)

with col1:
    # CSV de predicciones
    csv_predictions = df_result.to_csv(index=False)
    st.download_button(
        label="üì• Descargar Predicciones (CSV)",
        data=csv_predictions,
        file_name=f"predicciones_polinomicas_grado{degree}.csv",
        mime="text/csv",
        help="Descarga las predicciones del modelo para an√°lisis posterior"
    )

with col2:
    # CSV de comparaci√≥n de grados
    csv_comparison = df_comparison.to_csv(index=False)
    st.download_button(
        label="üì• Descargar Comparaci√≥n de Grados (CSV)",
        data=csv_comparison,
        file_name="comparacion_grados_polinomicos.csv",
        mime="text/csv",
        help="Descarga la tabla comparativa de diferentes grados polin√≥micos"
    )

# Guardar archivo localmente tambi√©n
df_result.to_csv("predicciones_polinomicas.csv", index=False)
st.success("‚úÖ Archivo 'predicciones_polinomicas.csv' guardado localmente para evaluaci√≥n posterior")

st.markdown("---")

# ============================================
# INFORMACI√ìN ADICIONAL
# ============================================
with st.expander("‚ÑπÔ∏è Acerca de la Regresi√≥n Polin√≥mica"):
    st.markdown("""
    ### ¬øQu√© es la Regresi√≥n Polin√≥mica?
    
    La regresi√≥n polin√≥mica es una extensi√≥n de la regresi√≥n lineal que permite modelar 
    relaciones no lineales entre las variables predictoras y la variable objetivo. 
    Esto se logra agregando t√©rminos polin√≥micos (potencias y productos) de las 
    caracter√≠sticas originales.
    
    ### F√≥rmula General
    
    Para una variable x y grado n:
    
    $y = Œ≤_0 + Œ≤_1x + Œ≤_2x^2 + Œ≤_3x^3 + ... + Œ≤_nx^n$
    
    ### Ventajas
    - Mayor flexibilidad para capturar relaciones no lineales
    - Sigue siendo un modelo lineal en los par√°metros (f√°cil de entrenar)
    - √ötil cuando la relaci√≥n entre variables es curvil√≠nea
    
    ### Desventajas
    - Propenso al sobreajuste con grados altos
    - El n√∫mero de caracter√≠sticas crece exponencialmente
    - Sensible a outliers
    
    ### Consejos para elegir el grado
    1. Comenzar con grado 2 y aumentar gradualmente
    2. Monitorear la diferencia entre R¬≤ de train y test
    3. Usar validaci√≥n cruzada para una evaluaci√≥n m√°s robusta
    4. Considerar t√©cnicas de regularizaci√≥n (Ridge, Lasso) si hay sobreajuste
    """)

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: gray;'>
    <p>Desarrollado para el curso de Algoritmos de Regresi√≥n</p>
    <p>Universidad Nacional de Trujillo - 2025</p>
</div>
""", unsafe_allow_html=True)
